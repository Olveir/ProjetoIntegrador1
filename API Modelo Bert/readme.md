ğŸ Guia de InstalaÃ§Ã£o e ExecuÃ§Ã£o - API Reptile 

ğŸ“‹ VisÃ£o Geral 

Esta API Flask Ã© um sistema de classificaÃ§Ã£o de texto usando modelos PyTorch (.pt) para categorizar textos em 10 categorias diferentes. O sistema foi projetado para trabalhar com modelos LLM salvos no formato PyTorch. 

ğŸ”§ PrÃ©-requisitos 

VersÃµes Recomendadas 

Python 3.8 ou superior 

Sistema operacional: Windows, Linux ou macOS 

Pelo menos 4GB de RAM livre 

EspaÃ§o em disco: 2GB para dependÃªncias 

Ferramentas NecessÃ¡rias 

Git (opcional, para controle de versÃ£o) 

Editor de cÃ³digo (VS Code, PyCharm, etc.) 

Terminal/Prompt de comando 

ğŸ“ Estrutura de Pastas NecessÃ¡ria 

Antes de comeÃ§ar, crie a seguinte estrutura de pastas: 

seu-projeto/ 

â”œâ”€â”€ app.py                     # Arquivo principal da API (seu cÃ³digo) 

â”œâ”€â”€ models/                    # Pasta para o modelo 

â”‚   â””â”€â”€ llama-pth-checkpoint/ 

â”‚       â””â”€â”€ melhor_modelo_pt.pt  # Seu modelo treinado 

â”œâ”€â”€ uploads/                   # Pasta para uploads (criada automaticamente) 

â”œâ”€â”€ requirements.txt           # DependÃªncias (serÃ¡ criado) 

â””â”€â”€ README.md                 # DocumentaÃ§Ã£o 

 

ğŸš€ Passo a Passo para InstalaÃ§Ã£o 

Passo 1: Preparar o Ambiente 

1.1 Criar pasta do projeto 

mkdir api-reptile 

cd api-reptile 

 

1.2 Criar ambiente virtual (OBRIGATÃ“RIO) 

# Windows 

python -m venv venv 

venv\Scripts\activate 

 

# Linux/macOS 

python3 -m venv venv 

source venv/bin/activate 

 

âš ï¸ IMPORTANTE: Sempre ative o ambiente virtual antes de instalar pacotes! 

Passo 2: Instalar DependÃªncias 

2.1 Criar arquivo requirements.txt 

Crie um arquivo chamado requirements.txt com o seguinte conteÃºdo: 

Flask==2.3.3 

Flask-CORS==4.0.0 

torch==2.0.1 

pandas==2.0.3 

numpy==1.24.3 

Werkzeug==2.3.7 

transformers==4.30.0 

 

âš ï¸ IMPORTANTE: A versÃ£o BERT requer a biblioteca transformers para o tokenizer! 

2.2 Instalar pacotes 

pip install -r requirements.txt 

 

Alternativa manual: 

pip install Flask Flask-CORS torch pandas numpy Werkzeug transformers 

 

âš ï¸ ATENÃ‡ÃƒO: Se vocÃª tiver problemas com a instalaÃ§Ã£o do transformers, tente: 

pip install transformers --no-cache-dir 

 

Passo 3: Preparar o Modelo 

3.1 Criar estrutura de pastas 

mkdir -p models/llama-pth-checkpoint 

 

3.2 Colocar o modelo 

Copie seu arquivo melhor_modelo_pt.pt para models/llama-pth-checkpoint/ 

Certifique-se de que o caminho estÃ¡ correto: models/llama-pth-checkpoint/melhor_modelo_pt.pt 

Passo 4: Configurar o CÃ³digo BERT 

4.1 Salvar o cÃ³digo corrigido 

IMPORTANTE: Use o cÃ³digo BERT corrigido (nÃ£o o cÃ³digo original com features manuais) 

Salve como app.py na pasta raiz do projeto 

4.2 Primeira execuÃ§Ã£o (Download do BERT) 

Na primeira execuÃ§Ã£o, o sistema baixarÃ¡ automaticamente: 

Modelo BERT: neuralmind/bert-base-portuguese-cased (~400MB) 

Tokenizer: VocabulÃ¡rio e configuraÃ§Ãµes 

# Primeira execuÃ§Ã£o pode demorar para baixar o BERT 

python app.py 

 

âš ï¸ IMPORTANTE: Certifique-se de ter conexÃ£o com internet estÃ¡vel! 

Passo 5: Testar a InstalaÃ§Ã£o 

5.1 Verificar dependÃªncias BERT 

python -c "import flask, torch, pandas, numpy, transformers; print('Todas as dependÃªncias BERT instaladas com sucesso!')" 

 

5.2 Testar o tokenizer BERT 

python -c "from transformers import BertTokenizer; tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased'); print('Tokenizer BERT funcionando!')" 

 

5.2 Verificar o modelo 

python -c "import os; print('Modelo encontrado:', os.path.exists('models/llama-pth-checkpoint/melhor_modelo_pt.pt'))" 

 

â–¶ï¸ Executando a API 

MÃ©todo 1: ExecuÃ§Ã£o Direta 

python app.py 

 

MÃ©todo 2: Usando Flask (alternativo) 

export FLASK_APP=app.py  # Linux/macOS 

set FLASK_APP=app.py     # Windows 

flask run --host=0.0.0.0 --port=5000 

 

ğŸ¯ Verificando se Funcionou 

Se tudo estiver correto, vocÃª verÃ¡ uma mensagem similar a: 

ğŸ API REPTILE CORRIGIDA - v2.0_fixed 

 

Status: âœ… OK 

Modelo: âœ… Carregado 

Categorias: 10 disponÃ­veis 

... 

* Running on all addresses (0.0.0.0) 

* Running on http://127.0.0.1:5000 

* Running on http://[seu-ip]:5000 

 

ğŸ§ª Testando a API 

Teste 1: Health Check 

curl http://localhost:5000/health 

 

Teste 2: Categorias DisponÃ­veis 

curl http://localhost:5000/categorias 

 

Teste 3: ClassificaÃ§Ã£o de Texto 

curl -X POST http://localhost:5000/classify \ 

  -H "Content-Type: application/json" \ 

  -d '{"text": "Como vocÃª estÃ¡ se sentindo hoje?"}' 

 

Teste 4: Teste ForÃ§ado (IMPORTANTE) 

curl -X POST http://localhost:5000/force-test \ 

  -H "Content-Type: application/json" \ 

  -d '{}' 

 

ğŸ”§ Endpoints da API BERT 

MÃ©todo 

Endpoint 

DescriÃ§Ã£o 

Novo/Modificado 

GET 

/ 

InformaÃ§Ãµes bÃ¡sicas da API BERT 

âœ… Atualizado 

GET 

/health 

Status BERT + Tokenizer 

âœ… Atualizado 

GET 

/categorias 

Lista de categorias terapÃªuticas 

âœ… Atualizado 

POST 

/classify 

Classifica texto (com contexto opcional) 

ğŸ†• Novo formato 

POST 

/classify-batch 

MÃºltiplos textos ou interaÃ§Ãµes 

ğŸ†• Novo formato 

POST 

/test-tokenizer 

Testa tokenizaÃ§Ã£o BERT 

ğŸ†• Novo 

POST 

/force-test-bert 

Teste forÃ§ado categorias especÃ­ficas 

ğŸ†• Novo 

POST 

/processar 

CSV com Fala_Terapeuta + Fala_Cliente 

âœ… Atualizado 

ğŸ“‹ Novos Formatos de RequisiÃ§Ã£o 

ClassificaÃ§Ã£o Individual: 

{ 

  "text": "Como vocÃª estÃ¡ se sentindo hoje?", 

  "contexto_cliente": "Estou ansioso" // Opcional 

} 

 

Lote Simples: 

{ 

  "texts": [ 

    "Como vocÃª estÃ¡?", 

    "Entendo sua dificuldade" 

  ] 

} 

 

Lote com Contexto: 

{ 

  "interacoes": [ 

    { 

      "texto": "Como vocÃª estÃ¡?", 

      "contexto_cliente": "Estou ansioso" 

    } 

  ] 

} 

 

âŒ Problemas Comuns BERT e SoluÃ§Ãµes 

Problema 1: "Tokenizer nÃ£o carregado" 

Causa: Biblioteca transformers nÃ£o instalada ou modelo BERT nÃ£o baixado SoluÃ§Ã£o: 

# Reinstalar transformers 

pip uninstall transformers 

pip install transformers --no-cache-dir 

 

# Verificar conexÃ£o de internet 

python -c "from transformers import BertTokenizer; BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')" 

 

Problema 2: "HTTP 404 - neuralmind/bert-base-portuguese-cased" 

Causa: Problema de conectividade com HuggingFace SoluÃ§Ã£o: 

# Usar proxy ou VPN se necessÃ¡rio 

export HF_ENDPOINT=https://huggingface.co 

 

# Download manual 

python -c " 

from transformers import BertTokenizer, BertForSequenceClassification 

tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased') 

model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=10) 

print('Download concluÃ­do!') 

" 

 

Problema 3: "BERT sempre retorna mesma categoria" 

Causa: Modelo .pt nÃ£o Ã© compatÃ­vel com BERT SoluÃ§Ã£o: 

Execute o teste forÃ§ado: POST /force-test-bert 

Verifique se o modelo foi treinado com BERT 

CRÃTICO: O modelo deve ter sido salvo apÃ³s treinamento com model.state_dict() 

Problema 4: "OutOfMemoryError" 

Causa: BERT consome muita RAM SoluÃ§Ã£o: 

# Reduzir batch size no cÃ³digo ou usar CPU 

export CUDA_VISIBLE_DEVICES=""  # ForÃ§ar CPU 

 

# Ou adicionar no cÃ³digo: 

device = torch.device("cpu")  # Sempre usar CPU 

 

Problema 5: "Erro ao carregar state_dict" 

Causa: Incompatibilidade entre modelo treinado e arquitetura SoluÃ§Ã£o: 

# Verificar as chaves do modelo salvo 

import torch 

checkpoint = torch.load('models/llama-pth-checkpoint/melhor_modelo_pt.pt', map_location='cpu') 

print("Tipo:", type(checkpoint)) 

if isinstance(checkpoint, dict): 

    print("Chaves:", list(checkpoint.keys())[:10]) 

 

ğŸ” DiagnÃ³stico AvanÃ§ado 

Verificar Logs 

# Executar com logs detalhados 

python app.py 2>&1 | tee app.log 

 

Testar Modelo Manualmente 

import torch 

 

# Carregar modelo 

model = torch.load('models/llama-pth-checkpoint/melhor_modelo_pt.pt', map_location='cpu') 

print(f"Tipo do modelo: {type(model)}") 

print(f"Chaves (se dict): {list(model.keys()) if isinstance(model, dict) else 'NÃ£o Ã© dict'}") 

 

Verificar Features 

# Testar extraÃ§Ã£o de features 

from app import extract_simple_features 

features = extract_simple_features("Texto de teste", target_size=3072) 

print(f"Shape: {features.shape}") 

print(f"Valores Ãºnicos: {len(torch.unique(features))}") 

 

ğŸ“ Uso em ProduÃ§Ã£o 

ConfiguraÃ§Ãµes Recomendadas 

# Substituir na linha final do app.py 

if __name__ == '__main__': 

    app.run( 

        debug=False,           # Desabilitar debug 

        host='0.0.0.0',       # Aceitar conexÃµes externas 

        port=5000,            # Porta padrÃ£o 

        threaded=True         # Permitir mÃºltiplas conexÃµes 

    ) 

 

Usando Gunicorn (Recomendado) 

pip install gunicorn 

gunicorn --bind 0.0.0.0:5000 --workers 4 app:app 

 

ğŸ” SeguranÃ§a 

ConfiguraÃ§Ãµes de SeguranÃ§a 

Desabilite CORS em produÃ§Ã£o (remova origins=["*"]) 

Use HTTPS em produÃ§Ã£o 

Implemente autenticaÃ§Ã£o se necessÃ¡rio 

Limite o tamanho dos uploads 

Exemplo de ConfiguraÃ§Ã£o Segura 

CORS(app, origins=["http://localhost:3000", "https://seudorminio.com"]) 

app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max 

 

ğŸ“Š Monitoramento 

Logs de ProduÃ§Ã£o 

import logging 

logging.basicConfig( 

    level=logging.INFO, 

    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', 

    handlers=[ 

        logging.FileHandler('api.log'), 

        logging.StreamHandler() 

    ] 

) 

 

ğŸ†˜ Suporte 

Checklist de VerificaÃ§Ã£o 

[ ] Python 3.8+ instalado 

[ ] Ambiente virtual ativado 

[ ] DependÃªncias instaladas 

[ ] Modelo na pasta correta 

[ ] PermissÃµes corretas 

[ ] Porta 5000 livre 

[ ] Teste forÃ§ado executado 

Comandos de Debug 

# Verificar versÃ£o Python 

python --version 

 

# Verificar pacotes instalados 

pip list 

 

# Verificar estrutura de pastas 

find . -type f -name "*.pt" 

 

# Verificar logs 

tail -f app.log 

 

 

ğŸ‰ Pronto para Usar! 

Se seguiu todos os passos, sua API BERT deve estar funcionando em: 

URL Local: http://localhost:5000 

Health Check: http://localhost:5000/health 

Interface HTML: Use o arquivo teste_api_bert.html fornecido 

âœ… Checklist Final BERT 

[ ] Python 3.8+ instalado 

[ ] Ambiente virtual ativado 

[ ] DependÃªncias instaladas (incluindo transformers) 

[ ] Modelo .pt na pasta correta 

[ ] Primeira execuÃ§Ã£o completada (BERT baixado) 

[ ] Teste de health retorna tokenizer_loaded: true 

[ ] Teste BERT forÃ§ado executado com sucesso 

ğŸ¯ PrÃ³ximos Passos BERT: 

Execute o teste BERT forÃ§ado (POST /force-test-bert) - CRÃTICO 

Teste com contexto - use cliente + terapeuta 

Processe CSVs com colunas Fala_Terapeuta e Fala_Cliente 

Use a interface HTML para testes visuais 

Configure para produÃ§Ã£o se aplicÃ¡vel 

ğŸš¨ Se Algo NÃ£o Funcionar: 

Verifique os logs para mensagens de erro 

Execute todos os testes da seÃ§Ã£o "Testando a API BERT" 

Confirme que o modelo foi treinado com BERT (nÃ£o features manuais) 

Verifique se hÃ¡ conexÃ£o com internet (para download do BERT) 

Lembre-se: Esta versÃ£o usa BERT portuguÃªs e Ã© incompatÃ­vel com modelos treinados com features manuais! 

 

ğŸ”„ MigraÃ§Ã£o da VersÃ£o Antiga 

Se vocÃª tinha a versÃ£o com features manuais: 

Backup do cÃ³digo antigo 

Substitua completamente pelo cÃ³digo BERT 

Retreine o modelo usando o script de treinamento BERT 

Teste a nova versÃ£o 

NÃ£o Ã© possÃ­vel usar modelos antigos com a nova API BERT! 

Boa sorte com sua API BERT! ğŸš€ğŸ§  

 
